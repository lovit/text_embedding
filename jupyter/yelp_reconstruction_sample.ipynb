{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../text_embedding/')\n",
    "import sklearn\n",
    "import text_embedding\n",
    "\n",
    "from text_embedding import Word2VecCorpus\n",
    "from text_embedding import Doc2VecCorpus\n",
    "from text_embedding import WordVectorInferenceDecorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_corpus_path=\"yelp_training_corpus.txt\"\n",
    "#word2vec_corpus = Word2VecCorpus(yelp_corpus_path, sent_delimiter='     ', num_doc=100000)\n",
    "word2vec_corpus=Doc2VecCorpus(yelp_corpus_path, num_doc=100000, sent_delimiter='     ', yield_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['050', 'tbone', 'grapes', 'nashville']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_terms=[]\n",
    "with open('./mask_candidates/sample/all_sample.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        masked_terms.append(line.rstrip())\n",
    "masked_terms[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_corpus = WordVectorInferenceDecorator(word2vec_corpus, test_terms=masked_terms, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('love', 'the', 'staff', 'love', 'the', 'meat', 'love', 'the', 'place', 'prepare', 'for', 'a', 'long', 'line', 'around', 'lunch', 'or', 'dinner', 'hours')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in inference_corpus:\n",
    "    print(sent, end='\\n\\n')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning vocabulary was done. 9957 terms from 107398 sents\n",
      "(word, context) was constructed from 107397 sents (9957 words, 0.714 Gb)\n",
      "train SVD ... done\n"
     ]
    }
   ],
   "source": [
    "from text_embedding import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(sentences=inference_corpus, size=300, window=4, min_count=20,\n",
    "    negative=10, alpha=0.0, beta=0.75, dynamic_weight=True,\n",
    "    verbose=True, n_iter=5, min_cooccurrence=3, prune_point=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(word, context) was constructed from 36544 sents (128 words, 0.500 Gb)\n",
      "128 terms are appended\n"
     ]
    }
   ],
   "source": [
    "inference_corpus.training = False\n",
    "infered_vec = word2vec.infer_wordvec(inference_corpus, masked_terms, append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning vocabulary was done. 13383 terms from 143943 sents\n",
      "(word, context) was constructed from 143942 sents (13383 words, 0.904 Gb)\n",
      "train SVD ... done\n"
     ]
    }
   ],
   "source": [
    "full_word2vec = Word2Vec(sentences=word2vec_corpus, size=300, window=4, min_count=20,\n",
    "    negative=10, alpha=0.0, beta=0.75, dynamic_weight=True,\n",
    "    verbose=True, n_iter=5, min_cooccurrence=3, prune_point=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating accuracy for top 1\n",
      "...Recon. Acc for top 1: 0.2578125\n",
      "Calculating accuracy for top 3\n",
      "...Recon. Acc for top 3: 0.5234375\n",
      "Calculating accuracy for top 5\n",
      "...Recon. Acc for top 5: 0.71875\n",
      "Calculating accuracy for top 10\n",
      "...Recon. Acc for top 10: 0.84375\n",
      "Calculating accuracy for top 30\n",
      "...Recon. Acc for top 30: 0.9921875\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "top_k=[1,3,5,10,30]\n",
    "\n",
    "recon_result=defaultdict(list)\n",
    "saver=0\n",
    "for ek in top_k:\n",
    "    acc=0\n",
    "    if ek == top_k[-1]:\n",
    "        saver=1\n",
    "    print(\"Calculating accuracy for top %s\" %str(ek))\n",
    "    for et in masked_terms:\n",
    "        w2v_base=[wpair[0] for wpair in full_word2vec.similar_words(et, topk=ek)]\n",
    "        w2v_recon=[wpair[0] for wpair in word2vec.similar_words(et, topk=ek)]\n",
    "        if not w2v_base or not w2v_recon: \n",
    "            print(\"Blank: %s\" %(et))\n",
    "        for recon_word in w2v_recon:\n",
    "            if recon_word in w2v_base:\n",
    "                acc+=1\n",
    "                break\n",
    "        if saver==1:\n",
    "            recon_result[et]=word2vec.similar_words(et, topk=ek)\n",
    "    print(\"...Recon. Acc for top %s: %s\" %(str(ek), str(float(acc/len(masked_terms)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('reconstruction_sample.pkl', 'wb') as f:\n",
    "    pickle.dump(recon_result, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
