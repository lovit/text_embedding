{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-20_article_all_normed_nountokenized.txt\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from config import corpus_path\n",
    "print(corpus_path.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30002"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import soynlp\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "\n",
    "sentences = DoublespaceLineCorpus(corpus_path, iter_sent=True)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordvec_infer import sents_to_word_contexts_matrix\n",
    "\n",
    "# x, idx2vocab = sents_to_word_contexts_matrix(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab2idx = {vocab:idx for idx, vocab in enumerate(idx2vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - counting word frequency from 30001 sents, mem=0.094 Gb #vocabs = 24906\n",
      "  - scanning (word, context) pairs from 30001 sents, mem=0.470 Gb\n",
      "Training PMI ... done\n",
      "Training SVD ... done\n"
     ]
    }
   ],
   "source": [
    "from wordvec_infer import Word2Vec\n",
    "\n",
    "def my_tokenizer(sent, stopwords={'아이오아이'}):\n",
    "    words = [word for word in sent.split() if not (word in stopwords)]\n",
    "    return words\n",
    "\n",
    "word2vec = Word2Vec(tokenizer=my_tokenizer, beta=1.0)\n",
    "word2vec.train(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('신용재', 0.91987635806576),\n",
       " ('타이틀곡', 0.914908670023072),\n",
       " ('백퍼센트', 0.8990926450997301),\n",
       " ('완전체', 0.8921406573384947),\n",
       " ('몬스타엑스', 0.8818118556260817),\n",
       " ('엠카운트다운', 0.8816437934129916),\n",
       " ('안무', 0.8779975011273335),\n",
       " ('열창', 0.8743633278054797),\n",
       " ('다비치', 0.8663248282954272),\n",
       " ('파이터', 0.8645118789327619),\n",
       " ('하이포', 0.8587448107377045),\n",
       " ('신곡', 0.8550658914947793),\n",
       " ('박진영', 0.8530412783291585),\n",
       " ('팡파레', 0.8433407143400704),\n",
       " ('수록곡', 0.8432095622258016),\n",
       " ('곡으로', 0.8402485071092015),\n",
       " ('레이디스코드', 0.8324961897393519),\n",
       " ('갓세븐', 0.8319702520657125),\n",
       " ('중독성', 0.8277036732753654),\n",
       " ('자작곡', 0.8247467952278615)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('너무너무너무', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('아이오아이')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24906, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24906, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec._components.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create (unseen word, contexts) matrix\n",
      "  - counting word frequency from 30001 sents, mem=0.366 Gb #vocabs = 24907\n",
      "  - scanning (word, context) pairs from 30001 sents, mem=0.366 Gb\n",
      "  - (word, context) matrix was constructed. shape = (24907, 23632)                    \n",
      "  - done\n"
     ]
    }
   ],
   "source": [
    "from wordvec_infer import sents_to_unseen_word_contexts_matrix\n",
    "\n",
    "X, idx2vocab = sents_to_unseen_word_contexts_matrix(\n",
    "    sentences, ['아이오아이'], word2vec._vocab2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = word2vec.inference_words(X, idx2vocab, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('신용재', 0.91987635806576),\n",
       " ('타이틀곡', 0.914908670023072),\n",
       " ('백퍼센트', 0.8990926450997301),\n",
       " ('완전체', 0.8921406573384947),\n",
       " ('몬스타엑스', 0.8818118556260817),\n",
       " ('엠카운트다운', 0.8816437934129916),\n",
       " ('안무', 0.8779975011273335),\n",
       " ('열창', 0.8743633278054797),\n",
       " ('다비치', 0.8663248282954272),\n",
       " ('파이터', 0.8645118789327619)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('너무너무너무')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24907, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24907, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv = word2vec.inference_words(X, idx2vocab)\n",
    "word2vec.wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24907, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv = word2vec.inference_words(X, idx2vocab)\n",
    "word2vec.wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2VecCorpus:\n",
    "    def __init__(self, path, num_doc=-1, verbose_point=-1):\n",
    "        self.path = path\n",
    "        self.num_doc = num_doc\n",
    "        self.verbose_point = verbose_point\n",
    "\n",
    "    def __iter__(self):\n",
    "        vp = self.verbose_point\n",
    "        with open(self.path, encoding='utf-8') as f:\n",
    "            for i, doc in enumerate(f):\n",
    "                if vp > 0 and i % vp == 0:\n",
    "                    print('\\riterating corpus ... %d lines' % (i+1), end='', flush=True)\n",
    "                if self.num_doc > 0 and i >= self.num_doc:\n",
    "                    break\n",
    "                doc = doc.strip()\n",
    "                if not doc:\n",
    "                    continue\n",
    "                streams = self._tokenize(i, doc)\n",
    "                for stream in streams:\n",
    "                    yield stream\n",
    "            if vp > 0:\n",
    "                print('\\riterating corpus was done%s' % (' '*20))\n",
    "\n",
    "    def _tokenize(self, i, doc):\n",
    "        for sent in doc.split('  '):\n",
    "            if sent:\n",
    "                yield sent.split()\n",
    "\n",
    "class Doc2VecCorpus(Word2VecCorpus):\n",
    "    def _tokenize(self, i, doc):\n",
    "        column = doc.split('\\t')\n",
    "        if len(column) == 1:\n",
    "            labels = ['__doc{}__'.format(i)]\n",
    "            words = column[0].split()\n",
    "        else:\n",
    "            labels = column[0].split()\n",
    "            words = [word for col in column[1:] for word in col.split()]\n",
    "        yield labels, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_path = '/mnt/lovit/works/fastcampus_text_ml/3rd/data/corpus_10days/news/2016-10-20_article_all_normed.txt'\n",
    "word2vec_corpus = Word2VecCorpus(corpus_path, num_doc=5, verbose_point=1)\n",
    "doc2vec_corpus = Doc2VecCorpus(corpus_path, num_doc=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['__doc1__'], ['19', '1990', '52', '1', '22'])\n",
      "(['__doc2__'], ['오패산터널', '총격전', '용의자', '검거', '서울', '연합뉴스', '경찰', '관계자들이', '19일', '오후', '서울', '강북구', '오패산', '터널', '인근에서', '사제', '총기를', '발사해', '경찰을', '살해한', '용의자', '성모씨를', '검거하고', '있다', '성씨는', '검거', '당시', '서바이벌', '게임에서', '쓰는', '방탄조끼에', '헬멧까지', '착용한', '상태였다', '독자제공', '영상', '캡처', '연합뉴스', '서울', '연합뉴스', '김은경', '기자', '사제', '총기로', '경찰을', '살해한', '범인', '성모', '46', '씨는', '주도면밀했다', '경찰에', '따르면', '성씨는', '19일', '오후', '강북경찰서', '인근', '부동산', '업소', '밖에서', '부동산업자', '이모', '67', '씨가', '나오기를', '기다렸다', '이씨와는', '평소에도', '말다툼을', '자주', '한', '것으로', '알려졌다', '이씨가', '나와', '걷기', '시작하자', '성씨는', '따라가면서', '미리', '준비해온', '사제', '총기를', '이씨에게', '발사했다', '총알이', '빗나가면서', '이씨는', '도망갔다', '그', '빗나간', '총알은', '지나가던', '행인', '71', '씨의', '배를', '스쳤다', '성씨는', '강북서', '인근', '치킨집까지', '이씨', '뒤를', '쫓으며', '실랑이하다', '쓰러뜨린', '후', '총기와', '함께', '가져온', '망치로', '이씨', '머리를', '때렸다', '이', '과정에서', '오후', '6시', '20분께', '강북구', '번동', '길', '위에서', '사람들이', '싸우고', '있다', '총소리가', '났다', '는', '등의', '신고가', '여러건', '들어왔다', '5분', '후에', '성씨의', '전자발찌가', '훼손됐다는', '신고가', '보호관찰소', '시스템을', '통해', '들어왔다', '성범죄자로', '전자발찌를', '차고', '있던', '성씨는', '부엌칼로', '직접', '자신의', '발찌를', '끊었다', '용의자', '소지', '사제총기', '2정', '서울', '연합뉴스', '임헌정', '기자', '서울', '시내에서', '폭행', '용의자가', '현장', '조사를', '벌이던', '경찰관에게', '사제총기를', '발사해', '경찰관이', '숨졌다', '19일', '오후', '6시28분', '강북구', '번동에서', '둔기로', '맞았다', '는', '폭행', '피해', '신고가', '접수돼', '현장에서', '조사하던', '강북경찰서', '번동파출소', '소속', '김모', '54', '경위가', '폭행', '용의자', '성모', '45', '씨가', '쏜', '사제총기에', '맞고', '쓰러진', '뒤', '병원에', '옮겨졌으나', '숨졌다', '사진은', '용의자가', '소지한', '사제총기', '신고를', '받고', '번동파출소에서', '김창호', '54', '경위', '등', '경찰들이', '오후', '6시', '29분께', '현장으로', '출동했다', '성씨는', '그사이', '부동산', '앞에', '놓아뒀던', '가방을', '챙겨', '오패산', '쪽으로', '도망간', '후였다', '김', '경위는', '오패산', '터널', '입구', '오른쪽의', '급경사에서', '성씨에게', '접근하다가', '오후', '6시', '33분께', '풀숲에', '숨은', '성씨가', '허공에', '난사한', '10여발의', '총알', '중', '일부를', '왼쪽', '어깨', '뒷부분에', '맞고', '쓰러졌다', '김', '경위는', '구급차가', '도착했을', '때', '이미', '의식이', '없었고', '심폐소생술을', '하며', '병원으로', '옮겨졌으나', '총알이', '폐를', '훼손해', '오후', '7시', '40분께', '사망했다', '김', '경위는', '외근용', '조끼를', '입고', '있었으나', '총알을', '막기에는', '역부족이었다', '머리에', '부상을', '입은', '이씨도', '함께', '병원으로', '이송됐으나', '생명에는', '지장이', '없는', '것으로', '알려졌다', '성씨는', '오패산', '터널', '밑쪽', '숲에서', '오후', '6시', '45분께', '잡혔다', '총격현장', '수색하는', '경찰들', '서울', '연합뉴스', '이효석', '기자', '19일', '오후', '서울', '강북구', '오패산', '터널', '인근에서', '경찰들이', '폭행', '용의자가', '사제총기를', '발사해', '경찰관이', '사망한', '사건을', '조사', '하고', '있다', '총', '때문에', '쫓던', '경관들과', '민간인들이', '몸을', '숨겼는데', '인근', '신발가게', '직원', '이모씨가', '다가가', '성씨를', '덮쳤고', '이어', '현장에', '있던', '다른', '상인들과', '경찰이', '가세해', '체포했다', '성씨는', '경찰에', '붙잡힌', '직후', '나', '자살하려고', '한', '거다', '맞아', '죽어도', '괜찮다', '고', '말한', '것으로', '전해졌다', '성씨', '자신도', '경찰이', '발사한', '공포탄', '1발', '실탄', '3발', '중', '실탄', '1발을', '배에', '맞았으나', '방탄조끼를', '입은', '상태여서', '부상하지는', '않았다', '경찰은', '인근을', '수색해', '성씨가', '만든', '사제총', '16정과', '칼', '7개를', '압수했다', '실제', '폭발할지는', '알', '수', '없는', '요구르트병에', '무언가를', '채워두고', '심지를', '꽂은', '사제', '폭탄도', '발견됐다', '일부는', '숲에서', '발견됐고', '일부는', '성씨가', '소지한', '가방', '안에', '있었다'])\n",
      "(['__doc3__'], ['테헤란', '연합뉴스', '강훈상', '특파원', '이용', '승객수', '기준', '세계', '최대', '공항인', '아랍에미리트', '두바이국제공항은', '19일', '현지시간', '이', '공항을', '이륙하는', '모든', '항공기의', '탑승객은', '삼성전자의', '갤럭시노트7을', '휴대하면', '안', '된다고', '밝혔다', '두바이국제공항은', '여러', '항공', '관련', '기구의', '권고에', '따라', '안전성에', '우려가', '있는', '스마트폰', '갤럭시노트7을', '휴대하고', '비행기를', '타면', '안', '된다', '며', '탑승', '전', '검색', '중', '발견되면', '압수할', '계획', '이라고', '발표했다', '공항', '측은', '갤럭시노트7의', '배터리가', '폭발', '우려가', '제기된', '만큼', '이', '제품을', '갖고', '공항', '안으로', '들어오지', '말라고', '이용객에', '당부했다', '이런', '조치는', '두바이국제공항', '뿐', '아니라', '신공항인', '두바이월드센터에도', '적용된다', '배터리', '폭발문제로', '회수된', '갤럭시노트7', '연합뉴스자료사진'])\n",
      "(['__doc4__'], ['브뤼셀', '연합뉴스', '김병수', '특파원', '독일', '정부는', '19일', '원자력발전소를', '폐쇄하기로', '함에', '따라', '원자력', '발전소', '운영자들에게', '핵폐기물', '처리를', '지원하는', '펀드에', '235억', '유로', '260억', '달러', '29조', '원', '를', '지불하도록', '하는', '계획을', '승인했다고', '언론들이', '보도했다', '앞서', '독일은', '5년', '전', '일본', '후쿠시마', '원전사태', '이후', '오는', '2022년까지', '원전', '17기를', '모두', '폐쇄하기로', '하고', '오는', '2050년까지', '전기생산량의', '80', '를', '재생에너지로', '충당하는', '것을', '목표로', '세웠다', '이날', '내각을', '통과한', '법안은', '원전', '운영자들이', '원전', '해체와', '핵폐기물', '처리를', '위한', '포장을', '책임지고', '정부는', '핵폐기물', '보관을', '책임지도록', '했다', '독일', '경제부는', '전력회사들과', '공식적인', '접촉은', '아직', '합의되지', '않았다고', '밝혔다', '독일', '원자력', '발전소', '연합뉴스', '자료사진'])\n"
     ]
    }
   ],
   "source": [
    "for doc in doc2vec_corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterating corpus was done                    \n"
     ]
    }
   ],
   "source": [
    "for sent in word2vec_corpus:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
